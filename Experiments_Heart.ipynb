{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Experimente - Datensatz Herzproblem"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Settings und Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#autoreload other packages when code changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-01T10:31:52.553635Z",
     "end_time": "2023-08-01T10:31:52.683376Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(42) #Reproduzierbarkeit\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from opacus import PrivacyEngine\n",
    "from opacus.accountants import RDPAccountant\n",
    "\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-01T10:31:52.569272Z",
     "end_time": "2023-08-01T10:32:00.717404Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from privacyflow.configs import path_configs\n",
    "from privacyflow.datasets import heart_dataset\n",
    "from privacyflow.preprocessing import heart_preprocess\n",
    "from privacyflow.models import heart_models"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-01T10:32:00.717404Z",
     "end_time": "2023-08-01T10:32:01.218310Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU will be used\n"
     ]
    }
   ],
   "source": [
    "#Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU will be used\")\n",
    "    device = torch.device('cpu')\n",
    "else:\n",
    "    print(\"No GPU available\")\n",
    "    device = torch.device('cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-01T10:32:01.274500Z",
     "end_time": "2023-08-01T10:32:01.286293Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Prep"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Der Heart-Datensatz ist eine einzelne .csv Datei. Diese wird mittels einer Preprocessing-Methode bereinigt und in Train-Val-Test gesplittet.\n",
    "Diese können anschließend als PyTorch Dataset genutzt werden."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "heart_preprocess.preprocess_heart_data()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-01T10:32:01.286293Z",
     "end_time": "2023-08-01T10:32:01.365686Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "train_dataset = heart_dataset.HeartDataset(mode=\"train\")\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_dataset = heart_dataset.HeartDataset(mode=\"val\")\n",
    "val_dataloader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_dataset = heart_dataset.HeartDataset(mode=\"test\")\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-01T10:32:01.356846Z",
     "end_time": "2023-08-01T10:32:01.417447Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model - Base"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Als Optimizer wird SGD mit Momentum genutzt, da dieser am ähnlichsten zum DPSGD ist.\n",
    "Andere Optimizer, z.B. Adam, erziehlen in weniger Epochen eine vergleichbare Güte"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "model_base = heart_models.HeartModelBase(13,2)\n",
    "model_base = model_base.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-01T10:32:01.391997Z",
     "end_time": "2023-08-01T10:32:01.422685Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_base.parameters(),lr=0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-01T10:32:01.422685Z",
     "end_time": "2023-08-01T10:32:01.427647Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1, Train Loss:0.44177, Val Acc:0.92473\n",
      "Epoch:  2, Train Loss:0.25355, Val Acc:0.92473\n",
      "Epoch:  3, Train Loss:0.18998, Val Acc:0.92473\n",
      "Epoch:  4, Train Loss:0.15231, Val Acc:0.91398\n",
      "Epoch:  5, Train Loss:0.17687, Val Acc:0.94624\n",
      "Epoch:  6, Train Loss:0.10023, Val Acc:0.97849\n",
      "Epoch:  7, Train Loss:0.03949, Val Acc:0.97849\n",
      "Epoch:  8, Train Loss:0.04541, Val Acc:0.95699\n",
      "Epoch:  9, Train Loss:0.11407, Val Acc:0.95699\n",
      "Epoch: 10, Train Loss:0.07807, Val Acc:0.95699\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model_base.train()\n",
    "    epoch_loss = 0.0\n",
    "    for batch in train_dataloader:\n",
    "        inputs,labels = batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_base(inputs)\n",
    "        loss = criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    model_base.eval()\n",
    "    num_correct = 0.0\n",
    "    for batch in val_dataloader:\n",
    "        inputs,labels = batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model_base(inputs)\n",
    "        _, predicted = torch.max(outputs,1)\n",
    "        num_correct += (predicted == labels).sum()\n",
    "    print(f\"Epoch: {epoch+1:2}, Train Loss:{epoch_loss/len(train_dataloader):.5f}, Val Acc:{num_correct/len(val_dataset):.5f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-01T10:32:01.434003Z",
     "end_time": "2023-08-01T10:32:02.816392Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9126)\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0.0\n",
    "model_base.eval()\n",
    "for batch in test_dataloader:\n",
    "    inputs,labels = batch\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    outputs = model_base(inputs)\n",
    "    _, predicted = torch.max(outputs,1)\n",
    "    num_correct += (predicted == labels).sum()\n",
    "\n",
    "accuracy_base_model = num_correct/len(test_dataset)\n",
    "print(accuracy_base_model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-01T10:32:02.816392Z",
     "end_time": "2023-08-01T10:32:02.862752Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Membership Inference Attacke"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need some test datapoints that must either be inside a dataset or not.\n",
    "For this we get the original train and test data and collect datapoints from there, since we know if they are inside the data or not"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(path_configs.HEART_DATA_TRAIN)\n",
    "val_data = pd.read_csv(path_configs.HEART_DATA_VAL)\n",
    "test_data = pd.read_csv(path_configs.HEART_DATA_TRAIN)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "dataset1 = pd.concat([train_data, val_data, test_data])\n",
    "dataset2 = pd.concat([train_data.tail(-100), test_data])\n",
    "dataset3 = pd.concat([train_data,val_data])\n",
    "dataset4 = pd.concat([train_data.tail(-100), val_data])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "torch_dataset1 = heart_dataset.HeartDataset(mode=\"custom\", custom_df=dataset1)\n",
    "dataloader1 = DataLoader(dataset=torch_dataset1,batch_size=32)\n",
    "\n",
    "torch_dataset2 = heart_dataset.HeartDataset(mode=\"custom\", custom_df=dataset2)\n",
    "dataloader2 = DataLoader(dataset=torch_dataset2,batch_size=32)\n",
    "\n",
    "torch_dataset3 = heart_dataset.HeartDataset(mode=\"custom\", custom_df=dataset3)\n",
    "dataloader3 = DataLoader(dataset=torch_dataset3,batch_size=32)\n",
    "\n",
    "torch_dataset4 = heart_dataset.HeartDataset(mode=\"custom\", custom_df=dataset4)\n",
    "dataloader4 = DataLoader(dataset=torch_dataset4,batch_size=32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Membership Inference Attack needs a bunch of shadow models, which are similar to the original model.\n",
    "Thus we use the same model arch and also a smaller and bigger version.\n",
    "Each of the models is trained twice, once with a specific datapoint and once without it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "#Train Shadow Models\n",
    "shadow_models = []\n",
    "for ds in [dataloader1,dataloader2,dataloader3,dataloader4]:\n",
    "    for size in ['base','base','base','base','small','large']:\n",
    "        if size == \"small\":\n",
    "            shadow_model = heart_models.HeartModelSmall(13,2).to(device)\n",
    "        elif size == \"large\":\n",
    "            shadow_model = heart_models.HeartModelLarge(13,2).to(device)\n",
    "        else:\n",
    "            shadow_model = heart_models.HeartModelBase(13,2).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(shadow_model.parameters(), lr=0.01)\n",
    "\n",
    "        for epoch in range(10):\n",
    "            for batch in ds:\n",
    "                inputs,labels = batch\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = shadow_model(inputs)\n",
    "                loss = criterion(outputs,labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        shadow_models.append(shadow_model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "#Construct Training Data for the Meta Classifier\n",
    "#It consits of the input (original data) and the logits of the shadow model and target=was_used_in_training\n",
    "dfs = []\n",
    "for i,mod in enumerate(shadow_models):\n",
    "    for mode in [\"train\", \"val\"]:\n",
    "        label = int(i < 12) if mode ==\"test\" else int(i < 6 or i >= 12) #Label if the data was included or not\n",
    "        df_inp = test_data.copy() if mode == \"test\" else val_data.copy()\n",
    "        ds = heart_dataset.HeartDataset(mode=\"custom\", custom_df=df_inp)\n",
    "        dl = DataLoader(dataset=ds,batch_size=1, shuffle=False)\n",
    "        prop1, prop2 = [], []\n",
    "        for inputs,labels in dl:\n",
    "            inputs = inputs.to(device)\n",
    "            preds = torch.softmax(mod(inputs),dim=1)\n",
    "            prop1.append(float(preds[0][0]))\n",
    "            prop2.append(float(preds[0][1]))\n",
    "        df_inp['logit1'] = prop1\n",
    "        df_inp['logit2'] = prop2\n",
    "        df_inp['target'] = label\n",
    "        dfs.append(df_inp)\n",
    "meta_classifier_data_df = pd.concat(dfs).reset_index()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "#Train Meta Classifier\n",
    "mi_model = heart_models.HeartMIModel(15,2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mi_model.parameters(),lr=0.01)\n",
    "\n",
    "for epoch in range(15):\n",
    "    mi_model.train()\n",
    "    for batch in train_dataloader:\n",
    "        inputs,labels = batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_base(inputs)\n",
    "        loss = criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "#Build Dataset for testing MI\n",
    "df_mi_eval = pd.concat([train_data.head(100),val_data])\n",
    "df_mi_ds = heart_dataset.HeartDataset(mode=\"custom\", custom_df=df_mi_eval)\n",
    "df_mi_dl = DataLoader(dataset=df_mi_ds,batch_size=1,shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def test_mi_attack(test_model) -> float:\n",
    "    #get predictions from original model\n",
    "    df_mi_model_data = df_mi_eval.copy()\n",
    "    prop1, prop2 = [], []\n",
    "    test_model.eval()\n",
    "    for inputs,labels in df_mi_dl:\n",
    "        inputs = inputs.to(device)\n",
    "        preds = torch.softmax(test_model(inputs),dim=1)\n",
    "        prop1.append(float(preds[0][0]))\n",
    "        prop2.append(float(preds[0][1]))\n",
    "    df_mi_model_data['logit1'] = prop1\n",
    "    df_mi_model_data['logit2'] = prop2\n",
    "    df_mi_model_data['target'] = [int(num<100) for num in range(len(df_mi_model_data.index))]\n",
    "\n",
    "    mi_ds = heart_dataset.MembershipInferenceDataset(df_mi_model_data)\n",
    "    mi_dl = DataLoader(dataset=mi_ds,batch_size=1,shuffle=False)\n",
    "\n",
    "    #test with mi model\n",
    "    num_correct_mi = 0.0\n",
    "    mi_model.eval()\n",
    "    for inputs,labels in mi_dl:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = mi_model(inputs)\n",
    "        _, predicted = torch.max(outputs,1)\n",
    "        num_correct_mi += (predicted == labels).sum()\n",
    "\n",
    "    accuracy_mi = num_correct_mi/len(df_mi_model_data.index)\n",
    "    print(f\"Accuracy MI:{accuracy_mi}\")\n",
    "    return accuracy_mi"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy MI:0.5388600826263428\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor(0.5389)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mi_attack(model_base)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model - DPSGD"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "model_dpsgd = heart_models.HeartModelBase(13,2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_dpsgd.parameters(),lr=0.01)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "privacy_engine = PrivacyEngine(\n",
    "    secure_mode=False,#Kryptographischer Zufall wird hier nicht benötigt\n",
    "    accountant=\"rdp\", #Renyi Differential Privacy\n",
    ")\n",
    "model_dpsgd, optimizer, train_dataloader = privacy_engine.make_private(\n",
    "    module=model_dpsgd,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=train_dataloader,\n",
    "    noise_multiplier=2.0, #Wie viel Rauschen wird hinzugefügt - Höher = weniger Rauschen\n",
    "    max_grad_norm=1.0 #Gradienten größer als dieser Wert werden geclippt\n",
    ")\n",
    "\n",
    "epsilon = 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1, Train Loss:0.63053, Val Acc:0.59140\n",
      "ε:0.5077396867750541\n",
      "Epoch:  2, Train Loss:0.53597, Val Acc:0.79570\n",
      "ε:0.6786679248122859\n",
      "Epoch:  3, Train Loss:0.51576, Val Acc:0.78495\n",
      "ε:0.8184904120481029\n",
      "Epoch:  4, Train Loss:0.49286, Val Acc:0.83871\n",
      "ε:0.9382069497545129\n",
      "Epoch:  5, Train Loss:0.58939, Val Acc:0.83871\n",
      "ε:1.0460510394184226\n",
      "Epoch:  6, Train Loss:0.56701, Val Acc:0.84946\n",
      "ε:1.1442454385839282\n",
      "Epoch:  7, Train Loss:0.56337, Val Acc:0.81720\n",
      "ε:1.2348563129090557\n",
      "Epoch:  8, Train Loss:0.63033, Val Acc:0.79570\n",
      "ε:1.320323189685948\n",
      "Epoch:  9, Train Loss:0.60279, Val Acc:0.81720\n",
      "ε:1.402160842402668\n",
      "Epoch: 10, Train Loss:0.67275, Val Acc:0.84946\n",
      "ε:1.4781191411143204\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    if privacy_engine.accountant.get_epsilon(delta=1e-5) > epsilon:\n",
    "        break\n",
    "    model_dpsgd.train()\n",
    "    epoch_loss = 0.0\n",
    "    for batch in train_dataloader:\n",
    "        inputs,labels = batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model_dpsgd(inputs)\n",
    "        loss = criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    model_dpsgd.eval()\n",
    "    num_correct = 0.0\n",
    "    for batch in val_dataloader:\n",
    "        inputs,labels = batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model_dpsgd(inputs)\n",
    "        _, predicted = torch.max(outputs,1)\n",
    "        num_correct += (predicted == labels).sum()\n",
    "    print(f\"Epoch: {epoch+1:2}, Train Loss:{epoch_loss/len(train_dataloader):.5f}, Val Acc:{num_correct/len(val_dataset):.5f}\")\n",
    "    print(f\"ε:{privacy_engine.accountant.get_epsilon(delta=1e-5)}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8155)\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0.0\n",
    "model_dpsgd.eval()\n",
    "for batch in test_dataloader:\n",
    "    inputs,labels = batch\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    outputs = model_dpsgd(inputs)\n",
    "    _, predicted = torch.max(outputs,1)\n",
    "    num_correct += (predicted == labels).sum()\n",
    "\n",
    "accuracy_dpsgd = num_correct/len(test_dataset)\n",
    "print(accuracy_dpsgd)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy MI:0.5440414547920227\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor(0.5440)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mi_attack(model_dpsgd)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
