{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Experimente - Datensatz Herzproblem"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Settings und Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-01T00:03:34.788364Z",
     "end_time": "2023-08-01T00:03:34.842363Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(42) #Reproduzierbarkeit\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from opacus import PrivacyEngine\n",
    "from opacus.accountants import RDPAccountant\n",
    "\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-01T00:03:34.798365Z",
     "end_time": "2023-08-01T00:03:38.048128Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from privacyflow.configs import path_configs\n",
    "from privacyflow.datasets import heart_dataset\n",
    "from privacyflow.preprocessing import heart_preprocess\n",
    "from privacyflow.models import heart_models"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-01T00:03:38.049128Z",
     "end_time": "2023-08-01T00:03:38.175097Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU will be used\n"
     ]
    }
   ],
   "source": [
    "#Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU will be used\")\n",
    "    device = torch.device('cpu')\n",
    "else:\n",
    "    print(\"No GPU available\")\n",
    "    device = torch.device('cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-01T00:03:38.195097Z",
     "end_time": "2023-08-01T00:03:38.235099Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Prep"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Der Heart-Datensatz ist eine einzelne .csv Datei. Diese wird mittels einer Preprocessing-Methode bereinigt und in Train-Val-Test gesplittet.\n",
    "Diese können anschließend als PyTorch Dataset genutzt werden."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "heart_preprocess.preprocess_heart_data()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-01T00:03:38.209098Z",
     "end_time": "2023-08-01T00:03:38.269099Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "train_dataset = heart_dataset.HeartDataset(mode=\"train\")\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_dataset = heart_dataset.HeartDataset(mode=\"val\")\n",
    "val_dataloader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_dataset = heart_dataset.HeartDataset(mode=\"test\")\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-01T00:03:38.271101Z",
     "end_time": "2023-08-01T00:03:38.316099Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model - Base"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Als Optimizer wird SGD mit Momentum genutzt, da dieser am ähnlichsten zum DPSGD ist.\n",
    "Andere Optimizer, z.B. Adam, erziehlen in weniger Epochen eine vergleichbare Güte"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "model_base = heart_models.HeartModelBase(13,2)\n",
    "model_base = model_base.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-01T00:03:38.317098Z",
     "end_time": "2023-08-01T00:03:38.332099Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_base.parameters(),lr=0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-01T00:03:38.333099Z",
     "end_time": "2023-08-01T00:03:38.365098Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1, Train Loss:0.43112, Val Acc:0.89024\n",
      "Epoch:  2, Train Loss:0.30492, Val Acc:0.86585\n",
      "Epoch:  3, Train Loss:0.22488, Val Acc:0.91463\n",
      "Epoch:  4, Train Loss:0.14747, Val Acc:0.93902\n",
      "Epoch:  5, Train Loss:0.15351, Val Acc:0.87805\n",
      "Epoch:  6, Train Loss:0.13302, Val Acc:0.95122\n",
      "Epoch:  7, Train Loss:0.07147, Val Acc:0.97561\n",
      "Epoch:  8, Train Loss:0.05319, Val Acc:0.97561\n",
      "Epoch:  9, Train Loss:0.01694, Val Acc:0.98780\n",
      "Epoch: 10, Train Loss:0.03525, Val Acc:0.97561\n",
      "Epoch: 11, Train Loss:0.10600, Val Acc:0.95122\n",
      "Epoch: 12, Train Loss:0.04796, Val Acc:0.98780\n",
      "Epoch: 13, Train Loss:0.07586, Val Acc:0.96341\n",
      "Epoch: 14, Train Loss:0.11121, Val Acc:0.96341\n",
      "Epoch: 15, Train Loss:0.02362, Val Acc:0.97561\n",
      "Epoch: 16, Train Loss:0.03842, Val Acc:0.98780\n",
      "Epoch: 17, Train Loss:0.01611, Val Acc:0.98780\n",
      "Epoch: 18, Train Loss:0.00273, Val Acc:0.98780\n",
      "Epoch: 19, Train Loss:0.02916, Val Acc:0.96341\n",
      "Epoch: 20, Train Loss:0.14180, Val Acc:0.90244\n",
      "Epoch: 21, Train Loss:0.05061, Val Acc:0.97561\n",
      "Epoch: 22, Train Loss:0.03099, Val Acc:0.95122\n",
      "Epoch: 23, Train Loss:0.03381, Val Acc:0.98780\n",
      "Epoch: 24, Train Loss:0.02419, Val Acc:0.97561\n",
      "Epoch: 25, Train Loss:0.01248, Val Acc:0.98780\n",
      "Epoch: 26, Train Loss:0.03992, Val Acc:0.98780\n",
      "Epoch: 27, Train Loss:0.00886, Val Acc:0.98780\n",
      "Epoch: 28, Train Loss:0.00238, Val Acc:0.98780\n",
      "Epoch: 29, Train Loss:0.00032, Val Acc:0.98780\n",
      "Epoch: 30, Train Loss:0.00003, Val Acc:0.98780\n",
      "Epoch: 31, Train Loss:0.00002, Val Acc:0.98780\n",
      "Epoch: 32, Train Loss:0.00002, Val Acc:0.98780\n",
      "Epoch: 33, Train Loss:0.00001, Val Acc:0.98780\n",
      "Epoch: 34, Train Loss:0.00001, Val Acc:0.98780\n",
      "Epoch: 35, Train Loss:0.00001, Val Acc:0.98780\n",
      "Epoch: 36, Train Loss:0.00001, Val Acc:0.98780\n",
      "Epoch: 37, Train Loss:0.00001, Val Acc:0.98780\n",
      "Epoch: 38, Train Loss:0.00001, Val Acc:0.98780\n",
      "Epoch: 39, Train Loss:0.00001, Val Acc:0.98780\n",
      "Epoch: 40, Train Loss:0.00000, Val Acc:0.98780\n",
      "Epoch: 41, Train Loss:0.00000, Val Acc:0.98780\n",
      "Epoch: 42, Train Loss:0.00000, Val Acc:0.98780\n",
      "Epoch: 43, Train Loss:0.00000, Val Acc:0.98780\n",
      "Epoch: 44, Train Loss:0.00000, Val Acc:0.98780\n",
      "Epoch: 45, Train Loss:0.00000, Val Acc:0.98780\n",
      "Epoch: 46, Train Loss:0.00000, Val Acc:0.98780\n",
      "Epoch: 47, Train Loss:0.00000, Val Acc:0.98780\n",
      "Epoch: 48, Train Loss:0.00000, Val Acc:0.98780\n",
      "Epoch: 49, Train Loss:0.00000, Val Acc:0.98780\n",
      "Epoch: 50, Train Loss:0.00000, Val Acc:0.98780\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    model_base.train()\n",
    "    epoch_loss = 0.0\n",
    "    for batch in train_dataloader:\n",
    "        inputs,labels = batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_base(inputs)\n",
    "        loss = criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    model_base.eval()\n",
    "    num_correct = 0.0\n",
    "    for batch in val_dataloader:\n",
    "        inputs,labels = batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model_base(inputs)\n",
    "        _, predicted = torch.max(outputs,1)\n",
    "        num_correct += (predicted == labels).sum()\n",
    "    print(f\"Epoch: {epoch+1:2}, Train Loss:{epoch_loss/len(train_dataloader):.5f}, Val Acc:{num_correct/len(val_dataset):.5f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-01T00:03:38.348098Z",
     "end_time": "2023-08-01T00:03:44.928364Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9756)\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0.0\n",
    "model_base.eval()\n",
    "for batch in test_dataloader:\n",
    "    inputs,labels = batch\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    outputs = model_base(inputs)\n",
    "    _, predicted = torch.max(outputs,1)\n",
    "    num_correct += (predicted == labels).sum()\n",
    "\n",
    "accuracy_base_model = num_correct/len(test_dataset)\n",
    "print(accuracy_base_model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-01T00:03:44.931364Z",
     "end_time": "2023-08-01T00:03:45.085365Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Membership Inference Attacke"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need some test datapoints that must either be inside a dataset or not.\n",
    "For this we get the original train and test data and collect datapoints from there, since we know if they are inside the data or not"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(path_configs.HEART_DATA_TRAIN)\n",
    "val_data = pd.read_csv(path_configs.HEART_DATA_VAL)\n",
    "test_data = pd.read_csv(path_configs.HEART_DATA_TRAIN)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-01T00:03:45.087365Z",
     "end_time": "2023-08-01T00:03:45.120364Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "dataset1 = pd.concat([train_data, val_data, test_data])\n",
    "dataset2 = pd.concat([train_data.tail(-100), test_data])\n",
    "dataset3 = pd.concat([train_data,val_data])\n",
    "dataset4 = pd.concat([train_data.tail(-100), val_data])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-01T00:03:45.121364Z",
     "end_time": "2023-08-01T00:03:45.132364Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "torch_dataset1 = heart_dataset.HeartDataset(mode=\"custom\", custom_df=dataset1)\n",
    "dataloader1 = DataLoader(dataset=torch_dataset1,batch_size=32)\n",
    "\n",
    "torch_dataset2 = heart_dataset.HeartDataset(mode=\"custom\", custom_df=dataset2)\n",
    "dataloader2 = DataLoader(dataset=torch_dataset2,batch_size=32)\n",
    "\n",
    "torch_dataset3 = heart_dataset.HeartDataset(mode=\"custom\", custom_df=dataset3)\n",
    "dataloader3 = DataLoader(dataset=torch_dataset3,batch_size=32)\n",
    "\n",
    "torch_dataset4 = heart_dataset.HeartDataset(mode=\"custom\", custom_df=dataset4)\n",
    "dataloader4 = DataLoader(dataset=torch_dataset4,batch_size=32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-01T00:03:45.132364Z",
     "end_time": "2023-08-01T00:03:45.162364Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Membership Inference Attack needs a bunch of shadow models, which are similar to the original model.\n",
    "Thus we use the same model arch and also a smaller and bigger version.\n",
    "Each of the models is trained twice, once with a specific datapoint and once without it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "#Train Shadow Models\n",
    "shadow_models = []\n",
    "for ds in [dataloader1,dataloader2,dataloader3,dataloader4]:\n",
    "    for size in ['base', 'small', 'large']:\n",
    "        if size == \"small\":\n",
    "            shadow_model = heart_models.HeartModelSmall(13,2).to(device)\n",
    "        elif size == \"large\":\n",
    "            shadow_model = heart_models.HeartModelLarge(13,2).to(device)\n",
    "        else:\n",
    "            shadow_model = heart_models.HeartModelBase(13,2).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(shadow_model.parameters(), lr=0.01)\n",
    "\n",
    "        for epoch in range(10):\n",
    "            for batch in ds:\n",
    "                inputs,labels = batch\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = shadow_model(inputs)\n",
    "                loss = criterion(outputs,labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        shadow_models.append(shadow_model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-01T00:03:45.154364Z",
     "end_time": "2023-08-01T00:04:08.395188Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "#Construct Training Data for the Meta Classifier\n",
    "#It consits of the input (original data) and the logits of the shadow model and target=was_used_in_training\n",
    "dfs = []\n",
    "for i,mod in enumerate(shadow_models):\n",
    "    for mode in [\"train\", \"val\"]:\n",
    "        label = int(i < 6) if mode ==\"test\" else int(i < 3 or i >= 6) #Label if the data was included or not\n",
    "        df_inp = test_data.copy() if mode == \"test\" else val_data.copy()\n",
    "        ds = heart_dataset.HeartDataset(mode=\"custom\", custom_df=df_inp)\n",
    "        dl = DataLoader(dataset=ds,batch_size=1, shuffle=False)\n",
    "        prop1, prop2 = [], []\n",
    "        for inputs,labels in dl:\n",
    "            inputs = inputs.to(device)\n",
    "            preds = torch.softmax(mod(inputs),dim=1)\n",
    "            prop1.append(float(preds[0][0]))\n",
    "            prop2.append(float(preds[0][1]))\n",
    "        df_inp['logit1'] = prop1\n",
    "        df_inp['logit2'] = prop2\n",
    "        df_inp['target'] = label\n",
    "        dfs.append(df_inp)\n",
    "meta_classifier_data_df = pd.concat(dfs).reset_index()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-01T00:04:08.399187Z",
     "end_time": "2023-08-01T00:04:09.066225Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "#Train Meta Classifier\n",
    "mi_model = heart_models.HeartMIModel(15,2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mi_model.parameters(),lr=0.01)\n",
    "\n",
    "for epoch in range(10):\n",
    "    mi_model.train()\n",
    "    for batch in train_dataloader:\n",
    "        inputs,labels = batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_base(inputs)\n",
    "        loss = criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-01T00:04:09.069225Z",
     "end_time": "2023-08-01T00:04:09.973228Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "#Build Dataset for testing MI\n",
    "df_mi_eval = pd.concat([train_data.head(100),val_data])\n",
    "df_mi_ds = heart_dataset.HeartDataset(mode=\"custom\", custom_df=df_mi_eval)\n",
    "df_mi_dl = DataLoader(dataset=df_mi_ds,batch_size=1,shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-01T00:04:09.975226Z",
     "end_time": "2023-08-01T00:04:09.989227Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def test_mi_attack(test_model) -> float:\n",
    "    #get predictions from original model\n",
    "    df_mi_model_data = df_mi_eval.copy()\n",
    "    prop1, prop2 = [], []\n",
    "    test_model.eval()\n",
    "    for inputs,labels in df_mi_dl:\n",
    "        inputs = inputs.to(device)\n",
    "        preds = torch.softmax(test_model(inputs),dim=1)\n",
    "        prop1.append(float(preds[0][0]))\n",
    "        prop2.append(float(preds[0][1]))\n",
    "    df_mi_model_data['logit1'] = prop1\n",
    "    df_mi_model_data['logit2'] = prop2\n",
    "    df_mi_model_data['target'] = [int(num<100) for num in range(len(df_mi_model_data.index))]\n",
    "\n",
    "    mi_ds = heart_dataset.MembershipInferenceDataset(df_mi_model_data)\n",
    "    mi_dl = DataLoader(dataset=mi_ds,batch_size=1,shuffle=False)\n",
    "\n",
    "    #test with mi model\n",
    "    num_correct_mi = 0.0\n",
    "    mi_model.eval()\n",
    "    for inputs,labels in mi_dl:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = mi_model(inputs)\n",
    "        _, predicted = torch.max(outputs,1)\n",
    "        num_correct_mi += (predicted == labels).sum()\n",
    "\n",
    "    accuracy_mi = num_correct_mi/len(df_mi_model_data.index)\n",
    "    print(f\"Accuracy MI:{accuracy_mi}\")\n",
    "    return accuracy_mi"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-01T00:04:09.995227Z",
     "end_time": "2023-08-01T00:04:10.019225Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy MI:0.5054945349693298\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor(0.5055)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mi_attack(model_base)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-01T00:09:27.297296Z",
     "end_time": "2023-08-01T00:09:27.416414Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model - DPSGD"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "model_dpsgd = heart_models.HeartModelBase(13,2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_dpsgd.parameters(),lr=0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-01T00:04:10.193226Z",
     "end_time": "2023-08-01T00:04:10.235225Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "privacy_engine = PrivacyEngine(\n",
    "    secure_mode=False,#Kryptographischer Zufall wird hier nicht benötigt\n",
    "    accountant=\"rdp\", #Renyi Differential Privacy\n",
    "\n",
    ")\n",
    "model_dpsgd, optimizer, train_dataloader = privacy_engine.make_private(\n",
    "    module=model_dpsgd,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=train_dataloader,\n",
    "    noise_multiplier=2.0, #Wie viel Rauschen wird hinzugefügt - Höher = weniger Rauschen\n",
    "    max_grad_norm=1.0 #Gradienten größer als dieser Wert werden geclippt\n",
    ")\n",
    "\n",
    "epsilon = 3"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-01T00:04:10.207225Z",
     "end_time": "2023-08-01T00:04:10.238225Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1, Train Loss:0.63525, Val Acc:0.82927\n",
      "ε:0.5339670390274576\n",
      "Epoch:  2, Train Loss:0.45643, Val Acc:0.87805\n",
      "ε:0.713145884425965\n",
      "Epoch:  3, Train Loss:0.39465, Val Acc:0.80488\n",
      "ε:0.8587803973789679\n",
      "Epoch:  4, Train Loss:0.40142, Val Acc:0.86585\n",
      "ε:0.9840949608505417\n",
      "Epoch:  5, Train Loss:0.49038, Val Acc:0.87805\n",
      "ε:1.0961219935999889\n",
      "Epoch:  6, Train Loss:0.56073, Val Acc:0.85366\n",
      "ε:1.1986428070986326\n",
      "Epoch:  7, Train Loss:0.44550, Val Acc:0.85366\n",
      "ε:1.2942849697674297\n",
      "Epoch:  8, Train Loss:0.54949, Val Acc:0.82927\n",
      "ε:1.38409397002675\n",
      "Epoch:  9, Train Loss:0.57724, Val Acc:0.81707\n",
      "ε:1.4678070509589602\n",
      "Epoch: 10, Train Loss:0.63696, Val Acc:0.86585\n",
      "ε:1.5493725448200941\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    if privacy_engine.accountant.get_epsilon(delta=1e-5) > epsilon:\n",
    "        break\n",
    "    model_dpsgd.train()\n",
    "    epoch_loss = 0.0\n",
    "    for batch in train_dataloader:\n",
    "        inputs,labels = batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model_dpsgd(inputs)\n",
    "        loss = criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    model_dpsgd.eval()\n",
    "    num_correct = 0.0\n",
    "    for batch in val_dataloader:\n",
    "        inputs,labels = batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model_dpsgd(inputs)\n",
    "        _, predicted = torch.max(outputs,1)\n",
    "        num_correct += (predicted == labels).sum()\n",
    "    print(f\"Epoch: {epoch+1:2}, Train Loss:{epoch_loss/len(train_dataloader):.5f}, Val Acc:{num_correct/len(val_dataset):.5f}\")\n",
    "    print(f\"ε:{privacy_engine.accountant.get_epsilon(delta=1e-5)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-01T00:04:10.225227Z",
     "end_time": "2023-08-01T00:04:15.065552Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7854)\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0.0\n",
    "model_dpsgd.eval()\n",
    "for batch in test_dataloader:\n",
    "    inputs,labels = batch\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    outputs = model_dpsgd(inputs)\n",
    "    _, predicted = torch.max(outputs,1)\n",
    "    num_correct += (predicted == labels).sum()\n",
    "\n",
    "accuracy_dpsgd = num_correct/len(test_dataset)\n",
    "print(accuracy_dpsgd)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-01T00:04:15.062568Z",
     "end_time": "2023-08-01T00:04:15.097659Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy MI:0.5219780206680298\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor(0.5220)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mi_attack(model_dpsgd)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-01T00:11:16.908583Z",
     "end_time": "2023-08-01T00:11:17.065595Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
