\chapter{Angriffe gegen Machine Learning Anwendungen}\label{sec:angriffe}

Neben den allgemeing√ºltigen Risiken gegen die Informationssicherheit gibt es eine Reihe spezifischer Risiken von Machine Learning Anwendungen.
Im Folgenden werden typische Angriffe gegen Machine Learning Modelle betrachtet und analysiert. 
Der Fokus liegt dabei auf Angriffen, welche besonders das Schutzziel Vertraulichkeit bei Neuronalen Netzen bedrohen.

\input{content/2_angriffe/deanonymisierung}
\input{content/2_angriffe/property_inference_attacke}
\input{content/2_angriffe/model_inversion_attacke}
\input{content/2_angriffe/membership_inference_attacke}
\input{content/2_angriffe/data_extraction}
\input{content/2_angriffe/poisoning_attacke}
\input{content/2_angriffe/verteiltes_lernen}
%\input{content/2_angriffe/weitere_angriffe}