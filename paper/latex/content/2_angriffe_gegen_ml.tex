\chapter{Angriffe gegen Machine Learning Anwendungen}\label{sec:angriffe}

Neben den allgemeingültigen Risiken gegen die Informationssicherheit gibt es eine Reihe spezifischer Risiken von Machine Learning Anwendungen.
Im Folgenden werden typische Angriffe gegen Machine Learning Modelle betrachtet und analysiert. 
Der Fokus liegt dabei auf Angriffen, welche besonders das Schutzziel der Vertraulichkeit bei neuronalen Netzen bedrohen.
Angriffe, welche nur die Verfügbarkeit oder die Güte eines Modells angreifen, werden hier außer Betracht gelassen.

\input{content/2_angriffe/hardware}
\input{content/2_angriffe/deanonymisierung}
\input{content/2_angriffe/model_inversion_attacke}
\input{content/2_angriffe/property_inference_attacke}
\input{content/2_angriffe/membership_inference_attacke}
\input{content/2_angriffe/data_extraction}
\input{content/2_angriffe/poisoning_attacke}
\input{content/2_angriffe/verteiltes_lernen}

\input{content/2_angriffe/zusammenfassung_angriffe}