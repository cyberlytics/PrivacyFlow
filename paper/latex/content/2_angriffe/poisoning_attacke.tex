\section{Poisoning Attacke}\label{sec:poisoning}

Bei der sogenannten Poisoning Attacke werden manipulierte Datensätze in die Trainingsdatenmenge eines Modells injiziert, wodurch das Modell schlechtere oder sogar falsche Vorhersagen trifft.
Ursprünglich ist diese Art des Angriffs recht populär bei Support Vektor Maschinen.
Biggio \etal \cite{P-15} zeigen, dass einige modifizierte Datenpunkte in der Nähe der Entscheidungsgrenze einer Support Vektor Maschine genügen, um die gelernte Funktion deutlich negativ zu beeinflussen.

Yang \etal \cite{P-17} zeigen ein Verfahren, bei dem eine Poisoning Attacke auf neuronalen Netzen angewendet wird.
Ziel hierbei ist es, die gefälschte Daten mit einem absichtlich falschen Label so zu wählen, dass der Wert der Verlustfunktion möglichst groß ist. 
Um dies zu erreichen, wird der Wert der Verlustfunktion des anzugreifenden Modells durch das Modell backpropagiert, wobei die gefälschten Daten als Teil des Modells betrachtet werden. 
Die Daten werden nun in Richtung der Gradienten angepasst, wodurch der Wert der Verlustfunktion steigt.
Werden die gefälschten Daten anschließend genutzt, um das Modell weiter zu trainieren, wird die Verlustfunktionen einen erhöhten Wert aufweisen, was zu einer stärkeren Anpassung des Modells führt, was aufgrund der gefälschten Label zu einer Verschlechterung der Güte des Modells führt.
Um nicht als gefälschte Daten aufzufallen, nutzen Yang \etal \cite{P-17} einen Autoencoder, der Daten so transformiert, dass diese vom Modell als echte Daten erkannt werden.
Damit der Autoencoder lernt, realistische Daten nachzubilden, wird ein zusätzliches Modell genutzt, welches einem Diskriminator der Generative Adversarial Network Architektur \cite{P-86} entspricht.

Guo und Liu \cite{P-16} nutzen einen Ansatz, bei welchem der Angreifer keinen Zugriff auf die Gradienten des angegriffenen Modells braucht.
Stattdessen wird ein vortrainiertes Modell genutzt, welches ähnlich zu dem angegriffenen Modell ist. 
Da diverse Modellarchitekturen Open-Source sind, finden sich auch einige vortrainierte Varianten von diesen im Internet.
Bei Bildklassifikation lässt sich beispielsweise ein vortrainiertes YOLO-Modell nutzen.
Dieses kann dann genutzt werden, um ein generatives Modell zu trainieren, welches wie bei Yang \etal \cite{P-17} die Gradienten des anzugreifenden Modells nutzt, um Daten für das anzugreifende Modell zu verschlimmern.
Guo und Liu \cite{P-16} gehen davon aus, dass das angegriffene Modell noch optimiert wurde und deshalb eine bessere Feature-Erkennung hat, als die öffentlich vortrainierten Modelle.
Dies macht den Angriff effektiver, sofern die Modelle nicht zu unterschiedlich sind.

Poisoning Attacken verschlechtern in der Regel lediglich die Performance eines Modells und sorgen für falsche Vorhersagen.
Tramèr \etal \cite{P-14} zeigen jedoch, dass manipulierte Daten dafür sorgen können, dass andere Angriffe, welche die Vertraulichkeit angreifen, effektiver werden können.
Durch Ändern des Labels eines Datensatzes kann dieser gegebenenfalls zu einem Ausreißer transformiert werden. 
Dadurch passt sich das Modell stärker diesem an, als wenn sich der Datenpunkt in die Messreihe einordnet.
Die falsche Klassifizierung des veränderten Datensatzes würde eine Memberhsip Inferenze Attacke auf diesen verbessern.
