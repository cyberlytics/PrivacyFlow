\section{Membership Inference Angriff}\label{sec:membership_inference}

Bei der Membership Inference Attacke versucht ein Angreifer herauszufinden, ob ein Datenpunkt Bestandteil der Trainingsdatenmenge eines Modells ist. 
Dies bedroht die Vertraulichkeit, da beispielsweise herausgefunden werden kann, ob eine bestimmte Person Teil eines Datenmenge für die Diagnose einer Krankheit ist und folglich auch mit der entsprechenden Krankheit diagnostiziert ist \cite{P-2}.

Shokri et al. \cite{P-2} zeigen eine Membership Inference Attacke, welche nur die Vorhersage des anzugreifenden Modells nutzt. 
Dies wird auch Black-Box Angriff genannt, da die internen Gewichte des Modells nichts ersichtlich sind und damit wie eine Art Black Box.
Die Attacke wird durchgeführt, indem eine Reihe Modelle trainiert wird, die ähnlich dem Angriffsziel-Modell sind.
Diese Modelle werden auch Shadow Modelle genannt.
Ähnlich bedeutet hier, dass sowohl die Architektur der Modelle, als auch der Trainingsdatenbestand, mit dem angegriffenen Modell vergleichbar sind.
Da es sich hier um einen Black-Box Angriff handelt, kann der Angreifer die Architektur nur anhand vergleichbarer Use Cases herleiten und schätzen. 
Als Trainingsdaten kann ein Angreifer öffentliche Datenbestände nutzen.
Für jedes Shadow Modell wird ein eigener Trainingsdatenbestand zusammengesetzt, wobei ein zu untersuchender Record in manchen dieser Bestände drinnen ist und in anderen wiederum nicht.
Somit haben einige der trainierten Shadow Modelle den zu untersuchenden Record in den Trainingsdaten, andere hingegen nicht.
Anschließend wird ein binärer Meta-Klassifikator trainiert (ähnlich zu der Property Inference Attacke in Kapitel \ref{sec:property_inference}), welcher anhand der Vorhersagen der Shadow Modelle, Label und Confidence Score, lernt, ob ein Record im Training des entsprechenden Modells genutzt wurde.
Wird in diesen Meta-Klassifikator die Vorhersage des angegriffenen Modells als Input genutzt, so erhält man die Antwort, ob der Record für das Modelltraining genutzt wurde oder nicht.
Laut Shokri et al. \cite{P-2} funktioniert dieser Angriff, da ähnliche Modelle, die mit einem ähnlichen Datenbestand trainiert werden, sich auch ähnlich verhalten. 
Somit kann bei den selbst trainierten Modellen, welche den zu untersuchenden Record enthalten, ein Muster gelernt werden, welches auch auf andere Modelle anwendbar ist. 
Overfitting eines Modells und eine hohe Anzahl an Klassen die vorhergesagt werden, erhöhen die Wahrscheinlichkeit eines erfolgreichen Angriffs auf ein Modell.



Carlini et al. \cite{P-13} zeigen eine Alternative des Angriffs, bei welcher kein Meta-Klassifikator genutzt wird.
Die Shadow Modelle werden analog zu \cite{P-2} trainiert. 
Anstatt mit den Vorhersagen dieser Modelle nun einen Meta-Klassifikator zu trainieren, werden zwei Gaußverteilungen gebildet, jeweils über die Confidence Scores der Modelle, wo der Datenpunkt im Training enthalten war oder nicht.
Mittels eines Likelihood-Quotienten-Tests wird anschließend vorhergesagt, in welcher Verteilung der Confidence Score des angegriffenen Modells wahrscheinlicher liegt.
Ein Vorteil dieser Methode liegt darin, dass weniger Shadow Modelle trainiert werden müssen, da bereits mit relativ wenig Werten eine Gaußverteilung modelliert werden kann.


Eine Voraussetzung bei Shokri et al. \cite{P-2} ist es, dass der Confidence Score mit ausgegeben wird.
Choquette-Choo et al. \cite{P-7} wandeln den Angriff ab, sodass dieser Score nicht mehr benötigt wird.
Der Angriff funktioniert simultan zu \cite{P-2}, jedoch wird nicht nur der Record selber in die Modelle als Input gegeben, sondern auch Abwandlung davon. 
Diese Abwandlungen könnte das Hinzufügen von zufälligem Rauschen sein, oder bei Bilddateien beispielsweise Rotation oder Translation.
Die Hypothese der Autoren ist, dass das Modell bei Records, die im Training genutzt wurden, robuster gegenüber diesen Abwandlungen ist und dennoch den Datenpunkt korrekt klassifiziert.
Zusätzlich könnten Abwandlungen der Daten über einen Data Augmentation Schritt auch direkt vom Modell gelernt worden sein.
Werden diese Abwandlungen also falsch klassifiziert, ist dies ein Indiz dafür, dass der Record nicht im Trainingsdatenbestand des anzugreifenden Modells ist.

